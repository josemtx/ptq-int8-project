\paragraph{Motivación}
La cuantización post-entrenamiento (PTQ) a 8 bits permite ejecutar redes con enteros, reduciendo memoria y ancho de banda y facilitando el despliegue en CPU/edge. El reto es mantener la precisión tras discretizar pesos y activaciones.

\paragraph{Objetivo}
Planteamos un problema de optimización: \emph{minimizar el tamaño del modelo} sujeto a la restricción de no degradar la precisión más de un umbral ($\Delta\mathrm{acc}\leq 2$ puntos). No perseguimos batir el estado del arte en accuracy, sino demostrar una solución heurística efectiva y reproducible.

\paragraph{Contribuciones}
(i) Un motor funcional \emph{QSim} en NumPy que simula inferencia INT8 (quant $\rightarrow$ INT32 $\rightarrow$ requant) con fusión Conv+ReLU; (ii) una heurística híbrida: \emph{greedy} por capa para pesos y enfriamiento simulado (SA) para percentiles de activación; (iii) una evaluación reproducible FP32 vs.\ INT8 en MNIST (precisión, latencia indicativa, tamaño), con reducción $\sim 4\times$ sin pérdida de precisión.
Véase \parencite{Jacob2018CVPR,Krishnamoorthi2018Whitepaper} para antecedentes de PTQ y \parencite{LeCun1998ProcIEEE,MNIST} para el conjunto MNIST.
