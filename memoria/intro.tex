\
        La implantación de modelos en dispositivos con recursos limitados motiva técnicas
        de compresión y aceleración. La cuantización post-entrenamiento (PTQ) a 8 bits es
        una de las más usadas por su simplicidad y por no requerir reentrenamiento.
        En esta memoria planteamos un micro-motor de ejecución cuantizada en NumPy y
        un problema de optimización: reducir la latencia manteniendo la precisión dentro de un margen.

        \textbf{Contribuciones}:
        (i) implementación educativa de PTQ con fusión Conv+ReLU;
        (ii) formulación de calibración como búsqueda heurística;
        (iii) evaluación sistemática con métricas reproducibles.
